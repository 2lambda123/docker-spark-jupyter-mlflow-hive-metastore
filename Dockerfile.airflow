FROM openjdk:8u242-jdk-stretch
COPY --from=python:3.7.6-slim-stretch / /
ENV DAEMON_RUN=true

RUN mkdir -p /usr/share/man/man1

RUN apt-get update && apt-get -y upgrade apt && \
    apt-get -y install software-properties-common build-essential \
      libkrb5-dev apt-transport-https curl openjdk-8-jdk gcc g++ git maven \
      python python-pip python-setuptools python-dev \
      r-base r-base-core rlwrap libopenblas-base libopenblas-dev \
      libfreetype6-dev gfortran musl libxslt1.1 libxslt1-dev \
      libxml2 libxml2-dev dumb-init jq && \
    rm -rf /var/lib/apt/lists/*

# Python Libraries
RUN python -m pip install --upgrade pip \
&&  python -m pip install pywinrm[kerberos] \
&&  python -m pip install texthero numpy \
torch torchvision tensorflow tensorboard boost xgboost catboost lightgbm \
tabulate mlxtend==0.18 hyperopt deap scikit-mdr skrebate yellowbrick spacy \
tpot update_checker arrow pyarrow lime shap plotly matplotlib bokeh mlflow \
gplearn imbalanced-learn tsfresh==0.17.0 sklearn-deap kmapper skope-rules delorean trimap \
pyldavis \
pyspark==2.4.7 \
pandas==1.1.5 \
scipy==1.5.4 \
tdigest==0.5.2.2 \
seaborn==0.11.0 \
scikit-learn==0.23.2 \
matplotlib==3.3.3 \
hurry.filesize==0.9 \
PyYAML==5.3.1 \
tqdm==4.54.1 \
zipp==0.6.0 \
urllib3==1.26.2 \
notebook==6.1.5 \
sqlalchemy==1.3.20 \
websocket_client==0.57.0 \
findspark
# --user

# Install SBT
ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64
ENV SBT_VERSION 0.13.15
RUN wget http://dl.bintray.com/sbt/debian/sbt-${SBT_VERSION}.deb -O /tmp/sbt.deb && \
    dpkg -i /tmp/sbt.deb && \
    rm -f /tmp/sbt.deb

# Install Spark
ARG SPARK_VERSION
ARG SPARK_HADOOP_VERSION
ENV SPARK_VERSION=${SPARK_VERSION}
ENV SPARK_HOME=/opt/spark-$SPARK_VERSION-bin-hadoop${SPARK_HADOOP_VERSION}
ENV SPARK_CONF_DIR=$SPARK_HOME/conf
ENV PATH $PATH:$SPARK_HOME/bin
RUN curl -sL \
  "https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop${SPARK_HADOOP_VERSION}.tgz" \
    | gunzip \
    | tar -x -C /opt/ \
  && chown -R root:root $SPARK_HOME \
  && mkdir -p /data/spark/ \
  && mkdir -p $SPARK_HOME/logs \
  && mkdir -p $SPARK_CONF_DIR \
  && chmod 777 $SPARK_HOME/logs 

# Install Readline Wrapper
RUN apt-get update && apt-get install -y rlwrap \
&&  rm -rf /var/lib/apt/lists/*

# Configure
ADD conf/hive/hive-site.xml $SPARK_CONF_DIR/

# Install Airflow
RUN pip install --no-cache-dir apache-airflow
RUN pip install --no-cache-dir --upgrade Flask

EXPOSE 8089
WORKDIR /airflow

COPY conf/airflow/entrypoint.sh /

ENV PATH="${SPARK_HOME}/bin:${PATH}"

ENV AIRFLOW__CORE__AIRFLOW_HOME=/airflow
ENV AIRFLOW__CORE__DAGS_FOLDER=/airflow/dags
ENV AIRFLOW__CORE__BASE_LOG_FOLDER=/airflow/logs
ENV AIRFLOW__CORE__PLUGINS_FOLDER=/airflow/plugins
ENV AIRFLOW__CORE__EXECUTOR=SequentialExecutor
ENV AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////airflow/airflow.db
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False

CMD ["airflow", "webserver", "--port", "8089"]
ENTRYPOINT ["/entrypoint.sh"]